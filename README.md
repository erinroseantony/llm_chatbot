# llm_chatbot
This code implements a simple LLM-based chatbot using Streamlit and the Ollama library. The chatbot allows users to input a prompt through a web interface and receive a response generated by a locally running language model, specifically "tinyllama" by default. The core function generate_llm_response formats the user's input into a message structure compatible with Ollama's API and retrieves the generated response. The Streamlit interface includes a title, instructions, a text area for input, and a button to trigger the response generation. It also provides visual feedback using a spinner during processing, displays the output in a text box, and includes error handling for failed responses or empty prompts. This setup is ideal for lightweight LLM experimentation or offline chatbot interaction through a clean and interactive UI.
